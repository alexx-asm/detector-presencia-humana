{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRX329vufiEE",
        "outputId": "b0636f7e-3713-4844-ac4f-15c37fb8d0c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYT66bOFoBtK",
        "outputId": "1f1a985b-42e1-40da-db50-d8722077af0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contenido de la carpeta data:\n",
            "['all_images', 'test_vigilancia']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/TFG/data'\n",
        "print(\"Contenido de la carpeta data:\")\n",
        "print(os.listdir(base_dir))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install Ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hNMai-kPoJP8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "import xml.etree.ElementTree as ET\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "import time\n",
        "import gc\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from ultralytics import YOLO\n",
        "import shutil\n",
        "from IPython.display import Image, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av83vkbQoPTp",
        "outputId": "1e8a52a6-9a72-4f8f-a92f-7e7eda70b22e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usando dispositivo: cuda\n",
            "Nombre de GPU: NVIDIA A100-SXM4-40GB\n",
            "VRAM disponible: 42.47 GB\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Usando dispositivo:\", device)\n",
        "print(\"Nombre de GPU:\", torch.cuda.get_device_name(0))\n",
        "print(f\"VRAM disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-TpKC28o3Ma",
        "outputId": "5367ea7a-6942-4b45-8eb1-a681c1f5d4b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directorio de imágenes: /content/drive/MyDrive/TFG/data/all_images/JPEGImages\n",
            "Directorio de anotaciones: /content/drive/MyDrive/TFG/data/all_images/Annotations\n"
          ]
        }
      ],
      "source": [
        "# Directorio raíz en Google Drive\n",
        "DATA_DIR = '/content/drive/MyDrive/TFG/data'\n",
        "\n",
        "# Usaremos solo esta carpeta con todas las imágenes y anotaciones juntas\n",
        "ALL_IMAGES_DIR = os.path.join(DATA_DIR, 'all_images')\n",
        "IMAGES_DIR = os.path.join(ALL_IMAGES_DIR, 'JPEGImages')\n",
        "ANNOTATIONS_DIR = os.path.join(ALL_IMAGES_DIR, 'Annotations')\n",
        "\n",
        "# Imágenes aparte para testear manualmente\n",
        "TEST_VIG_DIR = os.path.join(DATA_DIR, 'test_vigilancia')\n",
        "TEST_VIG_IMAGES_DIR = os.path.join(TEST_VIG_DIR, 'JPEGImages')\n",
        "TEST_VIG_ANNOTATIONS_DIR = os.path.join(TEST_VIG_DIR, 'Annotations')\n",
        "\n",
        "print(\"Directorio de imágenes:\", IMAGES_DIR)\n",
        "print(\"Directorio de anotaciones:\", ANNOTATIONS_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iXJpNiOo4S1",
        "outputId": "d655b6f8-7e67-45f5-e6ae-05dac346bc37"
      },
      "outputs": [],
      "source": [
        "# Listar todas las imágenes válidas\n",
        "all_image_files = glob.glob(os.path.join(IMAGES_DIR, '*.png')) + glob.glob(os.path.join(IMAGES_DIR, '*.jpg'))\n",
        "\n",
        "valid_image_files = []\n",
        "for img_path in all_image_files:\n",
        "    base_name = os.path.basename(img_path)\n",
        "    xml_name = os.path.splitext(base_name)[0] + \".xml\"\n",
        "    xml_path = os.path.join(ANNOTATIONS_DIR, xml_name)\n",
        "    if os.path.exists(xml_path):\n",
        "        valid_image_files.append(img_path)\n",
        "\n",
        "print(f\"Total imágenes válidas con anotación: {len(valid_image_files)}\")\n",
        "\n",
        "# Split en Train/Test\n",
        "train_ratio = 0.8\n",
        "test_ratio = 0.2\n",
        "\n",
        "train_files, test_files = train_test_split(\n",
        "    valid_image_files,\n",
        "    test_size=test_ratio,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Filtrando train_files para quitar imágenes sin cajas...\")\n",
        "\n",
        "filtered_train_files = []\n",
        "for img_path in tqdm(train_files):\n",
        "    xml_name = os.path.splitext(os.path.basename(img_path))[0] + \".xml\"\n",
        "    xml_path = os.path.join(ANNOTATIONS_DIR, xml_name)\n",
        "    try:\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "        # Buscamos al menos 1 objeto 'person'\n",
        "        found = any(\n",
        "            obj.find(\"name\").text.lower() == \"person\"\n",
        "            for obj in root.findall(\"object\")\n",
        "        )\n",
        "        if found:\n",
        "            filtered_train_files.append(img_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando {xml_path}: {e}\")\n",
        "\n",
        "print(f\"Train antes del filtro: {len(train_files)}\")\n",
        "print(f\"Train después del filtro: {len(filtered_train_files)}\")\n",
        "train_files = filtered_train_files\n",
        "\n",
        "# Guardar listas\n",
        "with open(\"train_files.txt\", \"w\") as f:\n",
        "    for path in train_files:\n",
        "        f.write(f\"{path}\\n\")\n",
        "\n",
        "with open(\"test_files.txt\", \"w\") as f:\n",
        "    for path in test_files:\n",
        "        f.write(f\"{path}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "pNig7RoOrGjK"
      },
      "outputs": [],
      "source": [
        "class CustomFRCNNDataset(Dataset):\n",
        "    def __init__(self, image_paths, annotations_dir, transforms=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.annotations_dir = annotations_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def parse_voc_xml(self, xml_file):\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for obj in root.findall(\"object\"):\n",
        "            if obj.find(\"name\").text.lower() != \"person\":\n",
        "                continue\n",
        "            bbox = obj.find(\"bndbox\")\n",
        "            xmin = int(float(bbox.find(\"xmin\").text))\n",
        "            ymin = int(float(bbox.find(\"ymin\").text))\n",
        "            xmax = int(float(bbox.find(\"xmax\").text))\n",
        "            ymax = int(float(bbox.find(\"ymax\").text))\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "            labels.append(1)  # Solo clase 'person'\n",
        "        return boxes, labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        xml_name = os.path.splitext(os.path.basename(img_path))[0] + \".xml\"\n",
        "        xml_path = os.path.join(self.annotations_dir, xml_name)\n",
        "        boxes, labels = self.parse_voc_xml(xml_path)\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'image_id': torch.tensor([idx])\n",
        "        }\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "wWRjiAJAsHP5"
      },
      "outputs": [],
      "source": [
        "# Transforms para Faster R-CNN\n",
        "transform_frcnn = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Dataset y DataLoader para Train\n",
        "train_dataset = CustomFRCNNDataset(train_files, ANNOTATIONS_DIR, transforms=transform_frcnn)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True,\n",
        "                          collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# Dataset y DataLoader para Test\n",
        "test_dataset = CustomFRCNNDataset(test_files, ANNOTATIONS_DIR, transforms=transform_frcnn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False,\n",
        "                         collate_fn=lambda x: tuple(zip(*x)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "AG6XF-fzsZjB",
        "outputId": "20d8c586-e766-4746-9b95-6134b3e5994c"
      },
      "outputs": [],
      "source": [
        "def show_sample_with_boxes(image_tensor, target):\n",
        "    img = F.to_pil_image(image_tensor)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img)\n",
        "    ax = plt.gca()\n",
        "    for box in target['boxes']:\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "        rect = patches.Rectangle(\n",
        "            (xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "            linewidth=2, edgecolor='red', facecolor='none'\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Visualizar la primera imagen del batch\n",
        "show_sample_with_boxes(images[0], targets[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WaBFlxaw8-H",
        "outputId": "a534e166-0de8-456b-a67d-b51792565599"
      },
      "outputs": [],
      "source": [
        "# Cargar Faster R-CNN preentrenado (en COCO)\n",
        "\n",
        "model_frcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model_frcnn.to(device)\n",
        "model_frcnn.eval()\n",
        "\n",
        "print(\"Modelo Faster R-CNN cargado y listo para inferencia.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "SdGsHJ_oyT8i"
      },
      "outputs": [],
      "source": [
        "# Función para leer XML\n",
        "def parse_voc_boxes(xml_path):\n",
        "    boxes = []\n",
        "    try:\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "        for obj in root.findall(\"object\"):\n",
        "            name = obj.find(\"name\").text\n",
        "            if name.lower() != \"person\":\n",
        "                continue\n",
        "            bbox = obj.find(\"bndbox\")\n",
        "            xmin = int(float(bbox.find(\"xmin\").text))\n",
        "            ymin = int(float(bbox.find(\"ymin\").text))\n",
        "            xmax = int(float(bbox.find(\"xmax\").text))\n",
        "            ymax = int(float(bbox.find(\"ymax\").text))\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing {xml_path}: {e}\")\n",
        "    return boxes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "y2fmT7xgy_iJ"
      },
      "outputs": [],
      "source": [
        "# Función para mostrar resultados\n",
        "def show_image_with_boxes(image_pil, pred_boxes, pred_scores, true_boxes, threshold=0.5):\n",
        "    plt.figure(figsize=(12,9))\n",
        "    plt.imshow(image_pil)\n",
        "    ax = plt.gca()\n",
        "\n",
        "    # Cajas reales en ROJO\n",
        "    for box in true_boxes:\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "        rect = patches.Rectangle(\n",
        "            (xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "            linewidth=2, edgecolor='red', facecolor='none'\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    # Predicciones en AZUL (con threshold)\n",
        "    for box, score in zip(pred_boxes, pred_scores):\n",
        "        if score >= threshold:\n",
        "            xmin, ymin, xmax, ymax = box\n",
        "            rect = patches.Rectangle(\n",
        "                (xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                linewidth=2, edgecolor='blue', facecolor='none'\n",
        "            )\n",
        "            ax.add_patch(rect)\n",
        "            ax.text(xmin, ymin - 5, f\"{score:.2f}\", color='blue', fontsize=9)\n",
        "\n",
        "    # Leyenda\n",
        "    handles = [\n",
        "        patches.Patch(edgecolor='red', facecolor='none', label='Real', linewidth=2),\n",
        "        patches.Patch(edgecolor='blue', facecolor='none', label='Predicha', linewidth=2)\n",
        "    ]\n",
        "    ax.legend(handles=handles, loc='upper right')\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ljxd3S_u0KSY",
        "outputId": "91d595d2-a35f-4780-a42d-f82772f1be3c"
      },
      "outputs": [],
      "source": [
        "# Inferencia sobre todas las imágenes de TEST_VIG\n",
        "\n",
        "for img_path in vig_image_files:\n",
        "    print(f\"\\nProcesando imagen: {os.path.basename(img_path)}\")\n",
        "\n",
        "    # Cargar imagen\n",
        "    image_pil = Image.open(img_path).convert(\"RGB\")\n",
        "    image_tensor = transform_frcnn(image_pil).to(device)\n",
        "\n",
        "    # Inferencia\n",
        "    with torch.no_grad():\n",
        "        prediction = model_frcnn([image_tensor])[0]\n",
        "\n",
        "    pred_boxes = prediction['boxes'].cpu()\n",
        "    pred_scores = prediction['scores'].cpu()\n",
        "    pred_labels = prediction['labels'].cpu()\n",
        "\n",
        "    # Filtrar SOLO clase 'person' (label=1)\n",
        "    person_mask = pred_labels == 1\n",
        "    pred_boxes = pred_boxes[person_mask]\n",
        "    pred_scores = pred_scores[person_mask]\n",
        "\n",
        "    # Cargar Annotations (XML)\n",
        "    xml_name = os.path.splitext(os.path.basename(img_path))[0] + \".xml\"\n",
        "    xml_path = os.path.join(TEST_VIG_ANNOTATIONS_DIR, xml_name)\n",
        "    true_boxes = parse_voc_boxes(xml_path)\n",
        "\n",
        "    # Mostrar imagen con cajas reales y predichas\n",
        "    show_image_with_boxes(\n",
        "        image_pil,\n",
        "        pred_boxes,\n",
        "        pred_scores,\n",
        "        true_boxes,\n",
        "        threshold=0.5\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY-fk1FA0r6P",
        "outputId": "9b82a115-28f9-48e4-d379-decdde410fcf"
      },
      "outputs": [],
      "source": [
        "# Para el entrenamiento\n",
        "val_ratio = 0.2\n",
        "val_size = int(len(train_dataset) * val_ratio)\n",
        "train_size = len(train_dataset) - val_size\n",
        "\n",
        "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"Train subset: {len(train_subset)} imágenes\")\n",
        "print(f\"Validation subset: {len(val_subset)} imágenes\")\n",
        "\n",
        "# Loaders\n",
        "train_loader_frcnn = DataLoader(train_subset, batch_size=8, shuffle=True,\n",
        "                                collate_fn=lambda x: tuple(zip(*x)))\n",
        "val_loader_frcnn = DataLoader(val_subset, batch_size=8, shuffle=False,\n",
        "                              collate_fn=lambda x: tuple(zip(*x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB2j2cI34rRA",
        "outputId": "8fdca4ac-c189-4d33-ce73-b0d7573a7c01"
      },
      "outputs": [],
      "source": [
        "print(model_frcnn.backbone)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_Ct7fGt42jf",
        "outputId": "8025cc43-4be5-4532-8f76-9e743cf9d497"
      },
      "outputs": [],
      "source": [
        "# Fine Tuning: descongelar solo layer4\n",
        "for name, param in model_frcnn.backbone.body.named_parameters():\n",
        "    if \"layer4\" in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Cabeza del detector (FastRCNNPredictor) ya está entrenable por defecto\n",
        "\n",
        "# Verificar\n",
        "total_params = sum(p.numel() for p in model_frcnn.parameters() if p.requires_grad)\n",
        "print(f\"Parámetros entrenables: {total_params}\")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam([p for p in model_frcnn.parameters() if p.requires_grad], lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGSNs6F76ZeQ",
        "outputId": "b866bde5-a0a4-479a-ecac-4c6dec68581c"
      },
      "outputs": [],
      "source": [
        "best_val_loss = float('inf')\n",
        "patience = 3\n",
        "counter = 0\n",
        "epochs = 100\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\n Epoch {epoch+1}/{epochs} iniciando...\")\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    # =======================\n",
        "    # TRAIN\n",
        "    # =======================\n",
        "    model_frcnn.train()\n",
        "    train_loss_epoch = 0\n",
        "    num_batches = len(train_loader_frcnn)\n",
        "\n",
        "    for batch_idx, (images, targets) in enumerate(train_loader_frcnn):\n",
        "        batch_start = time.time()\n",
        "\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        try:\n",
        "            loss_dict = model_frcnn(images, targets)\n",
        "            loss = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_time = time.time() - batch_start\n",
        "            train_loss_epoch += loss.item()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"   Batch {batch_idx}/{num_batches} - Loss: {loss.item():.4f} - {batch_time:.2f}s\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error en batch {batch_idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    avg_train_loss = train_loss_epoch / num_batches\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # =======================\n",
        "    # VALIDATION\n",
        "    # =======================\n",
        "    model_frcnn.train()\n",
        "    val_loss_epoch = 0\n",
        "    num_val_batches = len(val_loader_frcnn)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, targets) in enumerate(val_loader_frcnn):\n",
        "            batch_start = time.time()\n",
        "\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            try:\n",
        "                loss_dict = model_frcnn(images, targets)\n",
        "                loss = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "                batch_time = time.time() - batch_start\n",
        "                val_loss_epoch += loss.item()\n",
        "\n",
        "                if batch_idx % 10 == 0:\n",
        "                    print(f\"   Val Batch {batch_idx}/{num_val_batches} - Loss: {loss.item():.4f} - {batch_time:.2f}s\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\" Error en batch val {batch_idx}: {e}\")\n",
        "                continue\n",
        "\n",
        "    avg_val_loss = val_loss_epoch / num_val_batches\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    epoch_time = time.time() - epoch_start\n",
        "    print(f\"\\n Epoch {epoch+1} completada en {epoch_time:.2f}s\")\n",
        "    print(f\" Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # =======================\n",
        "    # Early Stopping\n",
        "    # =======================\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        counter = 0\n",
        "        torch.save(model_frcnn.state_dict(), \"best_fasterrcnn.pth\")\n",
        "        print(\" Mejor modelo guardado\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\" No mejora. EarlyStopping contador: {counter}/{patience}\")\n",
        "        if counter >= patience:\n",
        "            print(f\" Early stopping activado en epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    # =======================\n",
        "    # Limpieza de memoria\n",
        "    # =======================\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# =======================\n",
        "# Guardar histórico\n",
        "# =======================\n",
        "history_df = pd.DataFrame({\n",
        "    'epoch': range(1, len(train_losses)+1),\n",
        "    'train_loss': train_losses,\n",
        "    'val_loss': val_losses\n",
        "})\n",
        "history_df.to_csv(\"loss_history.csv\", index=False)\n",
        "print(\" Historial de pérdidas guardado en loss_history.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "6bL8bnsDMKER",
        "outputId": "9ccb5a04-c11d-4c10-b878-63f0383e938f"
      },
      "outputs": [],
      "source": [
        "# Loss VS Epoch\n",
        "\n",
        "# Cargar historial de pérdidas\n",
        "history = pd.read_csv(\"loss_history.csv\")\n",
        "\n",
        "# Gráfica\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(history['epoch'], history['train_loss'], label='Train Loss', marker='o')\n",
        "plt.plot(history['epoch'], history['val_loss'], label='Validation Loss', marker='s')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Evolución del Loss durante el entrenamiento')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "madNMG8dU4PO",
        "outputId": "7e947c30-5abb-4845-acf9-217d5b649437"
      },
      "outputs": [],
      "source": [
        "# Thresholds para evaluar (para la curva)\n",
        "thresholds = np.arange(0.1, 0.95, 0.05)\n",
        "\n",
        "# Almacenamos métricas por threshold\n",
        "all_precisions = []\n",
        "all_recalls = []\n",
        "all_APs = []\n",
        "\n",
        "all_iou_scores = []\n",
        "\n",
        "all_true_labels = []\n",
        "all_pred_labels = []\n",
        "\n",
        "inference_times = []\n",
        "\n",
        "print(f\"\\nEntorno de medición: Google Colab Pro con GPU {torch.cuda.get_device_name(0)}, batch_size=1\\n\")\n",
        "\n",
        "model_frcnn.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, targets in tqdm(test_loader, desc=\"Evaluando imágenes de TEST\"):\n",
        "        images = [img.to(device) for img in images]\n",
        "        true_boxes = targets[0]['boxes'].cpu().numpy()\n",
        "        true_label = 1 if len(true_boxes) > 0 else 0\n",
        "        all_true_labels.append(true_label)\n",
        "\n",
        "        # Medir tiempo\n",
        "        start_time = time.time()\n",
        "        outputs = model_frcnn(images)\n",
        "        end_time = time.time()\n",
        "\n",
        "        inference_times.append(end_time - start_time)\n",
        "\n",
        "        pred_boxes = outputs[0]['boxes'].cpu().numpy()\n",
        "        pred_scores = outputs[0]['scores'].cpu().numpy()\n",
        "        pred_labels = outputs[0]['labels'].cpu().numpy()\n",
        "\n",
        "        # Filtrar solo clase \"person\"\n",
        "        mask_person = pred_labels == 1\n",
        "        pred_boxes = pred_boxes[mask_person]\n",
        "        pred_scores = pred_scores[mask_person]\n",
        "\n",
        "        # Evaluar IoU promedio\n",
        "        for t_box in true_boxes:\n",
        "            best_iou = 0\n",
        "            for p_box in pred_boxes:\n",
        "                xA = max(t_box[0], p_box[0])\n",
        "                yA = max(t_box[1], p_box[1])\n",
        "                xB = min(t_box[2], p_box[2])\n",
        "                yB = min(t_box[3], p_box[3])\n",
        "                interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "                boxAArea = (t_box[2] - t_box[0]) * (t_box[3] - t_box[1])\n",
        "                boxBArea = (p_box[2] - p_box[0]) * (p_box[3] - p_box[1])\n",
        "                unionArea = boxAArea + boxBArea - interArea + 1e-6\n",
        "                iou = interArea / unionArea\n",
        "                best_iou = max(best_iou, iou)\n",
        "            all_iou_scores.append(best_iou)\n",
        "\n",
        "        # Para cada threshold, guardar label predicho (presencia/ausencia)\n",
        "        for thresh in thresholds:\n",
        "            pred_label = 1 if np.any(pred_scores >= thresh) else 0\n",
        "            all_pred_labels.append((thresh, pred_label))\n",
        "\n",
        "\n",
        "# =========================\n",
        "# IoU Promedio\n",
        "# =========================\n",
        "avg_iou_best_fasterrcnn = np.mean(all_iou_scores)\n",
        "print(f\"\\nIoU promedio en TEST: {avg_iou_best_fasterrcnn:.4f}\")\n",
        "\n",
        "# =========================\n",
        "# Precision y Recall para thresholds\n",
        "# =========================\n",
        "for thresh in thresholds:\n",
        "    pred_bin = [pred for t, pred in all_pred_labels if t == thresh]\n",
        "    TP = sum((np.array(pred_bin) == 1) & (np.array(all_true_labels) == 1))\n",
        "    FP = sum((np.array(pred_bin) == 1) & (np.array(all_true_labels) == 0))\n",
        "    FN = sum((np.array(pred_bin) == 0) & (np.array(all_true_labels) == 1))\n",
        "\n",
        "    precision = TP / (TP + FP + 1e-6)\n",
        "    recall = TP / (TP + FN + 1e-6)\n",
        "\n",
        "    all_precisions.append(precision)\n",
        "    all_recalls.append(recall)\n",
        "\n",
        "# Aproximar AP como área bajo Precision-Recall con thresholds como pasos\n",
        "all_APs = [p * r for p, r in zip(all_precisions, all_recalls)]\n",
        "mAP_best_fasterrcnn = np.mean(all_APs)\n",
        "print(f\"\\nAP aproximado (mAP) en TEST: {mAP_best_fasterrcnn:.4f}\")\n",
        "\n",
        "# =========================\n",
        "# Matriz de confusión para threshold óptimo\n",
        "# =========================\n",
        "# Elegir threshold óptimo\n",
        "best_thresh_idx = np.argmax(all_APs)\n",
        "best_threshold = thresholds[best_thresh_idx]\n",
        "print(f\"\\nThreshold óptimo encontrado: {best_threshold:.2f}\")\n",
        "\n",
        "final_preds = [pred for t, pred in all_pred_labels if t == best_threshold]\n",
        "cm = confusion_matrix(all_true_labels, final_preds)\n",
        "class_names = ['Sin persona', 'Persona']\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
        "plt.title('Matriz de Confusión en TEST')\n",
        "plt.show()\n",
        "\n",
        "# =========================\n",
        "# Gráfica Precision vs Threshold\n",
        "# =========================\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(thresholds, all_precisions, label='Precision', marker='o')\n",
        "plt.plot(thresholds, all_recalls, label='Recall', marker='x')\n",
        "plt.title(\"Precision y Recall vs. Threshold\")\n",
        "plt.xlabel(\"Threshold\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# =========================\n",
        "# Tiempo promedio de inferencia\n",
        "# =========================\n",
        "avg_inference_time_best_fasterrcnn = np.mean(inference_times)\n",
        "print(f\"\\nTiempo promedio de inferencia por imagen (batch_size=1, Google Colab Pro, GPU {torch.cuda.get_device_name(0)}): {avg_inference_time_best_fasterrcnn:.4f} segundos\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fEO8eWh7c9va",
        "outputId": "c82a43e4-fd7d-4a5a-fc15-b7b1c593dcc8"
      },
      "outputs": [],
      "source": [
        "# Inferencia sobre todas las imágenes de TEST_VIG\n",
        "\n",
        "for img_path in vig_image_files:\n",
        "    print(f\"\\nProcesando imagen: {os.path.basename(img_path)}\")\n",
        "\n",
        "    # Cargar imagen\n",
        "    image_pil = Image.open(img_path).convert(\"RGB\")\n",
        "    image_tensor = transform_frcnn(image_pil).to(device)\n",
        "\n",
        "    # Inferencia\n",
        "    with torch.no_grad():\n",
        "        prediction = model_frcnn([image_tensor])[0]\n",
        "\n",
        "    pred_boxes = prediction['boxes'].cpu()\n",
        "    pred_scores = prediction['scores'].cpu()\n",
        "    pred_labels = prediction['labels'].cpu()\n",
        "\n",
        "    # Filtrar SOLO clase 'person' (label=1)\n",
        "    person_mask = pred_labels == 1\n",
        "    pred_boxes = pred_boxes[person_mask]\n",
        "    pred_scores = pred_scores[person_mask]\n",
        "\n",
        "    # Cargar Annotations (XML)\n",
        "    xml_name = os.path.splitext(os.path.basename(img_path))[0] + \".xml\"\n",
        "    xml_path = os.path.join(TEST_VIG_ANNOTATIONS_DIR, xml_name)\n",
        "    true_boxes = parse_voc_boxes(xml_path)\n",
        "\n",
        "    # Mostrar imagen con cajas reales y predichas\n",
        "    show_image_with_boxes(\n",
        "        image_pil,\n",
        "        pred_boxes,\n",
        "        pred_scores,\n",
        "        true_boxes,\n",
        "        threshold=best_threshold\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EiNa-EVShyWl"
      },
      "outputs": [],
      "source": [
        "BASE_DRIVE = '/content/drive/MyDrive/TFG'\n",
        "YOLO_DATA_DIR = os.path.join(BASE_DRIVE, 'yolo_data')\n",
        "TEST_VIG_DIR = os.path.join(DATA_DIR, 'test_vigilancia')\n",
        "\n",
        "def voc_to_yolo(xml_path, img_w, img_h):\n",
        "    yolo_lines = []\n",
        "    try:\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "        for obj in root.findall(\"object\"):\n",
        "            if obj.find(\"name\").text.lower() != \"person\":\n",
        "                continue\n",
        "            bbox = obj.find(\"bndbox\")\n",
        "            xmin = float(bbox.find(\"xmin\").text)\n",
        "            ymin = float(bbox.find(\"ymin\").text)\n",
        "            xmax = float(bbox.find(\"xmax\").text)\n",
        "            ymax = float(bbox.find(\"ymax\").text)\n",
        "\n",
        "            cx = ((xmin + xmax) / 2) / img_w\n",
        "            cy = ((ymin + ymax) / 2) / img_h\n",
        "            w = (xmax - xmin) / img_w\n",
        "            h = (ymax - ymin) / img_h\n",
        "\n",
        "            yolo_lines.append(f\"0 {cx:.6f} {cy:.6f} {w:.6f} {h:.6f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing {xml_path}: {e}\")\n",
        "    return yolo_lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "-BvMOFzgsJRr"
      },
      "outputs": [],
      "source": [
        "def process_split(split_name, file_list_path):\n",
        "    with open(file_list_path, 'r') as f:\n",
        "        image_paths = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    images_out = os.path.join(YOLO_DATA_DIR, 'images', split_name)\n",
        "    labels_out = os.path.join(YOLO_DATA_DIR, 'labels', split_name)\n",
        "\n",
        "    count_total = 0\n",
        "    count_converted = 0\n",
        "\n",
        "    for img_path in image_paths:\n",
        "        base_name = os.path.basename(img_path)\n",
        "        xml_name = os.path.splitext(base_name)[0] + '.xml'\n",
        "        xml_path = os.path.join(DATA_DIR, 'all_images', 'Annotations', xml_name)\n",
        "\n",
        "        if not os.path.exists(xml_path):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with Image.open(img_path) as im:\n",
        "                w, h = im.size\n",
        "        except Exception as e:\n",
        "            print(f\"Error opening image {img_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Convertir\n",
        "        yolo_lines = voc_to_yolo(xml_path, w, h)\n",
        "        if len(yolo_lines) == 0:\n",
        "            continue  # filtrar sin boxes\n",
        "\n",
        "        # Copiar imagen\n",
        "        shutil.copy2(img_path, os.path.join(images_out, base_name))\n",
        "\n",
        "        # Guardar label\n",
        "        txt_name = os.path.splitext(base_name)[0] + '.txt'\n",
        "        txt_path = os.path.join(labels_out, txt_name)\n",
        "        with open(txt_path, 'w') as f:\n",
        "            f.write(\"\\n\".join(yolo_lines))\n",
        "\n",
        "        count_converted += 1\n",
        "        count_total += 1\n",
        "\n",
        "    print(f\"Procesadas {count_total} imágenes. Convertidas con cajas: {count_converted}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLdIuNEOsZQO",
        "outputId": "921e90bb-7671-4832-a55f-c414555d5701"
      },
      "outputs": [],
      "source": [
        "process_split('train', '/content/train_files.txt')\n",
        "process_split('val', '/content/test_files.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_htFE6ZyswEO",
        "outputId": "20bc9617-3ba7-4a07-9dff-4da34fa5eba1"
      },
      "outputs": [],
      "source": [
        "TEST_VIG_YOLO = os.path.join(BASE_DRIVE, 'test_vigilancia_yolo')\n",
        "\n",
        "TEST_VIG_YOLO_IMAGES = os.path.join(TEST_VIG_YOLO, 'images')\n",
        "TEST_VIG_YOLO_LABELS = os.path.join(TEST_VIG_YOLO, 'labels')\n",
        "\n",
        "test_vig_imgs = [f for f in os.listdir(TEST_VIG_IMAGES_DIR) if f.lower().endswith(('.jpg', '.png'))]\n",
        "\n",
        "print(f\"Total imágenes en TEST_VIG: {len(test_vig_imgs)}\")\n",
        "\n",
        "converted_count = 0\n",
        "\n",
        "for img_file in test_vig_imgs:\n",
        "    img_path = os.path.join(TEST_VIG_IMAGES_DIR, img_file)\n",
        "    xml_path = os.path.join(TEST_VIG_ANNOTATIONS_DIR, os.path.splitext(img_file)[0] + '.xml')\n",
        "\n",
        "    try:\n",
        "        with Image.open(img_path) as im:\n",
        "            w, h = im.size\n",
        "    except:\n",
        "        print(f\"Error abriendo {img_file}\")\n",
        "        continue\n",
        "\n",
        "    # Convertir a formato YOLO\n",
        "    yolo_lines = voc_to_yolo(xml_path, w, h)\n",
        "\n",
        "    # Copiar SIEMPRE la imagen\n",
        "    shutil.copy2(img_path, os.path.join(TEST_VIG_YOLO_IMAGES, img_file))\n",
        "\n",
        "    # Guardar TXT (vacío si no hay cajas)\n",
        "    txt_name = os.path.splitext(img_file)[0] + '.txt'\n",
        "    txt_path = os.path.join(TEST_VIG_YOLO_LABELS, txt_name)\n",
        "    with open(txt_path, 'w') as f:\n",
        "        f.write(\"\\n\".join(yolo_lines))\n",
        "\n",
        "    converted_count += 1\n",
        "\n",
        "print(f\"Conversión completada: {converted_count} imágenes procesadas en test_vigilancia.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kL_yktqyuwFA",
        "outputId": "1ef5ca3a-36e8-470a-d671-ddf3bbb92a90"
      },
      "outputs": [],
      "source": [
        "yaml_content = \"\"\"\n",
        "path: /content/drive/MyDrive/TFG/yolo_data\n",
        "train: images/train\n",
        "val: images/val\n",
        "\n",
        "nc: 1\n",
        "names: ['person']\n",
        "\"\"\"\n",
        "\n",
        "with open('/content/drive/MyDrive/TFG/yolo_data/data.yaml', 'w') as f:\n",
        "    f.write(yaml_content.strip())\n",
        "\n",
        "print(\"data.yaml generado en /content/drive/MyDrive/TFG/yolo_data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mjzhPi8WvTK7"
      },
      "outputs": [],
      "source": [
        "# Cargar modelo preentrenado de Ultralytics\n",
        "yolov8m = YOLO('yolov8m.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Upa7oMo5wcd0",
        "outputId": "b350c6f3-48b7-4dbd-8ba6-bff86a5ca536"
      },
      "outputs": [],
      "source": [
        "# Entrenar (fine-tuning)\n",
        "yolov8m.train(\n",
        "    data='/content/drive/MyDrive/TFG/yolo_data/data.yaml',\n",
        "    epochs=100,\n",
        "    imgsz=800,\n",
        "    batch=4,\n",
        "    lr0=0.0001,\n",
        "    optimizer='Adam',\n",
        "    patience=3,\n",
        "    workers=2,           # Menos workers para evitar RAM del sistema\n",
        "    project='/content/drive/MyDrive/TFG/yolo_data',\n",
        "    name='train_yolov8m',\n",
        "    device=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "iaJCJJdA11wu",
        "outputId": "17fd5ec6-3ea9-4424-bcae-9769a349af68"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/MyDrive/TFG/yolo_data/train_yolov8m2/weights/best.pt'\n",
        "best_yolov8m = YOLO(model_path)\n",
        "Image('/content/drive/MyDrive/TFG/yolo_data/train_yolov8m2/results.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RhA3idv1v5Bt",
        "outputId": "6cf080a0-cd89-48f2-c9d9-7675a5c859f9"
      },
      "outputs": [],
      "source": [
        "results = best_yolov8m.val(\n",
        "    data='/content/drive/MyDrive/TFG/yolo_data/data.yaml',\n",
        "    project=\"/content/drive/MyDrive/TFG/yolo_data\",\n",
        "    name=\"val_manual\",\n",
        "    imgsz=1024,\n",
        "    batch=8\n",
        ")\n",
        "Image('/content/drive/MyDrive/TFG/yolo_data/val_manual/confusion_matrix.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2vI-NC7pz8ac",
        "outputId": "6683c1c6-7635-43ae-d996-97fbd59379f8"
      },
      "outputs": [],
      "source": [
        "TEST_VIG_YOLO_IMAGES_DIR = '/content/drive/MyDrive/TFG/test_vigilancia_yolo/images'\n",
        "TEST_VIG_YOLO_LABELS_DIR = '/content/drive/MyDrive/TFG/test_vigilancia_yolo/labels'\n",
        "\n",
        "def load_yolo_labels(label_path, img_w, img_h):\n",
        "    \"\"\"\n",
        "    Lee un archivo txt de YOLO y devuelve las cajas en formato xyxy\n",
        "    \"\"\"\n",
        "    boxes = []\n",
        "    if not os.path.exists(label_path):\n",
        "        return boxes\n",
        "    with open(label_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 5:\n",
        "                continue\n",
        "            # YOLO format: class cx cy w h (normalized)\n",
        "            cls, cx, cy, w, h = map(float, parts)\n",
        "            xmin = (cx - w/2) * img_w\n",
        "            ymin = (cy - h/2) * img_h\n",
        "            xmax = (cx + w/2) * img_w\n",
        "            ymax = (cy + h/2) * img_h\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "    return boxes\n",
        "\n",
        "\n",
        "def show_image_with_yolo_preds_and_truth(image, pred_boxes, true_boxes, pred_scores, threshold=0.1):\n",
        "    plt.figure(figsize=(12,9))\n",
        "    plt.imshow(image)\n",
        "    ax = plt.gca()\n",
        "\n",
        "    # Dibujar cajas reales en rojo\n",
        "    for box in true_boxes:\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                 linewidth=2, edgecolor='red', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    # Dibujar predicciones en azul\n",
        "    for box, score in zip(pred_boxes, pred_scores):\n",
        "        if score < threshold:\n",
        "            continue\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                 linewidth=2, edgecolor='blue', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(xmin, ymin - 5, f\"{score:.2f}\", color='blue', fontsize=9)\n",
        "\n",
        "    # Leyenda\n",
        "    handles = [\n",
        "        patches.Patch(edgecolor='red', facecolor='none', label='Real', linewidth=2),\n",
        "        patches.Patch(edgecolor='blue', facecolor='none', label='Predicha', linewidth=2)\n",
        "    ]\n",
        "    ax.legend(handles=handles, loc='upper right')\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Listar imágenes\n",
        "test_vig_image_files = sorted([\n",
        "    os.path.join(TEST_VIG_YOLO_IMAGES_DIR, f)\n",
        "    for f in os.listdir(TEST_VIG_YOLO_IMAGES_DIR)\n",
        "    if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "])\n",
        "\n",
        "print(f\"Total imágenes a procesar: {len(test_vig_image_files)}\")\n",
        "\n",
        "for img_path in test_vig_image_files:\n",
        "    print(f\"Procesando imagen: {os.path.basename(img_path)}\")\n",
        "\n",
        "    # Cargar imagen\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    img_h, img_w = img_rgb.shape[:2]\n",
        "\n",
        "    # Inferencia\n",
        "    results = best_yolov8m(img_rgb)\n",
        "    boxes_xyxy = results[0].boxes.xyxy.cpu().numpy()\n",
        "    scores = results[0].boxes.conf.cpu().numpy()\n",
        "    labels = results[0].boxes.cls.cpu().numpy()\n",
        "\n",
        "    # Filtrar solo la clase 'person' (asumimos clase 0)\n",
        "    mask_person = labels == 0\n",
        "    pred_boxes = boxes_xyxy[mask_person]\n",
        "    pred_scores = scores[mask_person]\n",
        "\n",
        "    # Cargar cajas reales\n",
        "    label_txt = os.path.join(TEST_VIG_YOLO_LABELS_DIR, os.path.splitext(os.path.basename(img_path))[0] + \".txt\")\n",
        "    true_boxes = load_yolo_labels(label_txt, img_w, img_h)\n",
        "\n",
        "    # Mostrar imagen con cajas\n",
        "    show_image_with_yolo_preds_and_truth(\n",
        "        img_rgb,\n",
        "        pred_boxes,\n",
        "        true_boxes,\n",
        "        pred_scores,\n",
        "        threshold=0.1  # usar tu threshold óptimo si quieres\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0owDEO3kpV7J",
        "outputId": "96da3a70-e474-4f6b-c9dd-1dae78e3b05a"
      },
      "outputs": [],
      "source": [
        "TEST_VIG_YOLO_IMAGES_DIR = '/content/drive/MyDrive/TFG/test_vigilancia_yolo/images'\n",
        "TEST_VIG_YOLO_LABELS_DIR = '/content/drive/MyDrive/TFG/test_vigilancia_yolo/labels'\n",
        "\n",
        "def load_yolo_labels(label_path, img_w, img_h):\n",
        "    \"\"\"\n",
        "    Lee un archivo txt de YOLO y devuelve las cajas en formato xyxy\n",
        "    \"\"\"\n",
        "    boxes = []\n",
        "    if not os.path.exists(label_path):\n",
        "        return boxes\n",
        "    with open(label_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 5:\n",
        "                continue\n",
        "            # YOLO format: class cx cy w h (normalized)\n",
        "            cls, cx, cy, w, h = map(float, parts)\n",
        "            xmin = (cx - w/2) * img_w\n",
        "            ymin = (cy - h/2) * img_h\n",
        "            xmax = (cx + w/2) * img_w\n",
        "            ymax = (cy + h/2) * img_h\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "    return boxes\n",
        "\n",
        "\n",
        "def show_image_with_yolo_preds_and_truth(image, pred_boxes, true_boxes, pred_scores, threshold=0.1):\n",
        "    plt.figure(figsize=(12,9))\n",
        "    plt.imshow(image)\n",
        "    ax = plt.gca()\n",
        "\n",
        "    # Dibujar cajas reales en rojo\n",
        "    for box in true_boxes:\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                 linewidth=2, edgecolor='red', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    # Dibujar predicciones en azul\n",
        "    for box, score in zip(pred_boxes, pred_scores):\n",
        "        if score < threshold:\n",
        "            continue\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                 linewidth=2, edgecolor='blue', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(xmin, ymin - 5, f\"{score:.2f}\", color='blue', fontsize=9)\n",
        "\n",
        "    # Leyenda\n",
        "    handles = [\n",
        "        patches.Patch(edgecolor='red', facecolor='none', label='Real', linewidth=2),\n",
        "        patches.Patch(edgecolor='blue', facecolor='none', label='Predicha', linewidth=2)\n",
        "    ]\n",
        "    ax.legend(handles=handles, loc='upper right')\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Listar imágenes\n",
        "test_vig_image_files = sorted([\n",
        "    os.path.join(TEST_VIG_YOLO_IMAGES_DIR, f)\n",
        "    for f in os.listdir(TEST_VIG_YOLO_IMAGES_DIR)\n",
        "    if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "])\n",
        "\n",
        "print(f\"Total imágenes a procesar: {len(test_vig_image_files)}\")\n",
        "\n",
        "model = YOLO('yolov8m.pt')\n",
        "print(\"Modelo YOLOv8m preentrenado cargado.\")\n",
        "\n",
        "for img_path in test_vig_image_files:\n",
        "    print(f\"Procesando imagen: {os.path.basename(img_path)}\")\n",
        "\n",
        "    # Cargar imagen\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    img_h, img_w = img_rgb.shape[:2]\n",
        "\n",
        "    # Inferencia\n",
        "    results = model(img_rgb)\n",
        "    boxes_xyxy = results[0].boxes.xyxy.cpu().numpy()\n",
        "    scores = results[0].boxes.conf.cpu().numpy()\n",
        "    labels = results[0].boxes.cls.cpu().numpy()\n",
        "\n",
        "    # Filtrar solo la clase 'person' (asumimos clase 0)\n",
        "    mask_person = labels == 0\n",
        "    pred_boxes = boxes_xyxy[mask_person]\n",
        "    pred_scores = scores[mask_person]\n",
        "\n",
        "    # Cargar cajas reales\n",
        "    label_txt = os.path.join(TEST_VIG_YOLO_LABELS_DIR, os.path.splitext(os.path.basename(img_path))[0] + \".txt\")\n",
        "    true_boxes = load_yolo_labels(label_txt, img_w, img_h)\n",
        "\n",
        "    # Mostrar imagen con cajas\n",
        "    show_image_with_yolo_preds_and_truth(\n",
        "        img_rgb,\n",
        "        pred_boxes,\n",
        "        true_boxes,\n",
        "        pred_scores,\n",
        "        threshold=0.1  # usar tu threshold óptimo si quieres\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
